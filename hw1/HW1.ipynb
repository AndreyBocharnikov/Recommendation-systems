{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOI4Ir119/ZHbFG50Vny4o+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreyBocharnikov/Recommendation-systems/blob/master/hw1/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_6TzKJ3pOP9",
        "outputId": "cf2dc599-71ed-49d2-bd64-bdab14827d35"
      },
      "source": [
        "!pip install implicit\r\n",
        "!pip install lightfm\r\n",
        "\r\n",
        "import implicit\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import scipy.sparse as sp\r\n",
        "\r\n",
        "from lightfm.datasets import fetch_movielens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting implicit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/07/c0121884722d16e2c5beeb815f6b84b41cbf22e738e4075f1475be2791bc/implicit-0.4.4.tar.gz (1.1MB)\n",
            "\r\u001b[K     |▎                               | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 20.3MB/s eta 0:00:01\r\u001b[K     |▉                               | 30kB 24.8MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40kB 28.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51kB 19.0MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61kB 21.1MB/s eta 0:00:01\r\u001b[K     |██                              | 71kB 16.2MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81kB 12.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92kB 13.4MB/s eta 0:00:01\r\u001b[K     |███                             | 102kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122kB 13.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153kB 13.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 174kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 204kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 245kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 276kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 389kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 419kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 481kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 512kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 542kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 573kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 604kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 634kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 665kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 696kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 757kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 778kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 788kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 808kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 819kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 839kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 849kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 870kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921kB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 931kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952kB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 962kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983kB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 993kB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 13.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1MB 13.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from implicit) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from implicit) (1.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from implicit) (4.41.1)\n",
            "Building wheels for collected packages: implicit\n",
            "  Building wheel for implicit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for implicit: filename=implicit-0.4.4-cp37-cp37m-linux_x86_64.whl size=3403169 sha256=bff7b8921e2781dc8514d6e60b446fb6f8b4f5516b589aeac4e782ccf3a81f35\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/d4/ec/fd4f622fcbefb7521f149905295b2c26adecb23af38aa28217\n",
            "Successfully built implicit\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.4.4\n",
            "Collecting lightfm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/fe/8864d723daa8e5afc74080ce510c30f7ad52facf6a157d4b42dec83dfab4/lightfm-1.16.tar.gz (310kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 12.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from lightfm) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from lightfm) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightfm) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->lightfm) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightfm) (1.0.1)\n",
            "Building wheels for collected packages: lightfm\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=706127 sha256=5327eec8ac7129afb072fe51e948bb123ca6d540618c33614fad0a309625d4b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/c6/64/d4/673c7277f71ac4c5ad4835b94708c01b653ef2d3aa78ef20aa\n",
            "Successfully built lightfm\n",
            "Installing collected packages: lightfm\n",
            "Successfully installed lightfm-1.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM0vYOTRvIRM",
        "outputId": "f4496215-5f4d-4e3d-86bd-97b30b8b724f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive/', force_remount=True)\r\n",
        "\r\n",
        "dir_name = '/content/drive/My Drive/itmo/RecSys/datasets/ml-1m'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBEASGYTvz03"
      },
      "source": [
        "import os\r\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjHE1YjPf5CG"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "import torch.optim as optim\r\n",
        "import numpy.random\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDYeHseB1IF"
      },
      "source": [
        "##Подготовка данных и вспомогательные функции"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAtNO6DEEd6l"
      },
      "source": [
        "Прочитаем данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "n9hFzkTgtuXm",
        "outputId": "15e043ad-8aea-480e-b9cd-7e44be77d3f2"
      },
      "source": [
        "ratings = pd.read_csv(os.path.join(dir_name, 'ratings.dat'), delimiter='::', header=None, \r\n",
        "        names=['user_id', 'movie_id', 'rating', 'timestamp'], \r\n",
        "        usecols=['user_id', 'movie_id', 'rating'], engine='python')\r\n",
        "print(ratings.shape)\r\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000209, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  movie_id  rating\n",
              "0        1      1193       5\n",
              "1        1       661       3\n",
              "2        1       914       3\n",
              "3        1      3408       4\n",
              "4        1      2355       5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "Y3rty0Vywvrl",
        "outputId": "814690bf-7c10-4316-820b-b8beeec89445"
      },
      "source": [
        "movie_info = pd.read_csv(os.path.join(dir_name, 'movies.dat'), delimiter='::', header=None, \r\n",
        "        names=['movie_id', 'name', 'category'], engine='python')\r\n",
        "print(movie_info.shape)\r\n",
        "movie_info.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3883, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>movie_id</th>\n",
              "      <th>name</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Toy Story (1995)</td>\n",
              "      <td>Animation|Children's|Comedy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Jumanji (1995)</td>\n",
              "      <td>Adventure|Children's|Fantasy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Grumpier Old Men (1995)</td>\n",
              "      <td>Comedy|Romance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Waiting to Exhale (1995)</td>\n",
              "      <td>Comedy|Drama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Father of the Bride Part II (1995)</td>\n",
              "      <td>Comedy</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   movie_id                                name                      category\n",
              "0         1                    Toy Story (1995)   Animation|Children's|Comedy\n",
              "1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\n",
              "2         3             Grumpier Old Men (1995)                Comedy|Romance\n",
              "3         4            Waiting to Exhale (1995)                  Comedy|Drama\n",
              "4         5  Father of the Bride Part II (1995)                        Comedy"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7pnvH7J4UqZ"
      },
      "source": [
        "Преобразуем данные: для user'ов и item'ов перейдём от id к index'ам, чтобы было удобней обращаться по индексу к их bias'ам и эмбеддингам."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "ydj-KDq84J7K",
        "outputId": "cab4e841-8725-4cde-c19d-e6e9620fd112"
      },
      "source": [
        "unique_users = np.unique(ratings.iloc[:, 0])\r\n",
        "n_unique_users = len(unique_users)\r\n",
        "print(\"Unique users:\", unique_users, len(unique_users))\r\n",
        "userid_to_userindex = {}\r\n",
        "userindex_to_userid = {}\r\n",
        "for i, user in enumerate(unique_users):\r\n",
        "  userid_to_userindex[user] = i\r\n",
        "  userindex_to_userid[i] = user \r\n",
        "\r\n",
        "unique_movies = np.unique(ratings.iloc[:, 1])\r\n",
        "n_unique_movies = len(unique_movies)\r\n",
        "print(\"Unique items:\", unique_movies, len(unique_movies))\r\n",
        "movieid_to_movieindex = {}\r\n",
        "movieindex_to_movieid = {}\r\n",
        "for i, movie in enumerate(unique_movies):\r\n",
        "  movieid_to_movieindex[movie] = i\r\n",
        "  movieindex_to_movieid[i] = movie \r\n",
        "\r\n",
        "ratings['user_index'] = ratings.user_id.apply(lambda x: userid_to_userindex.get(x))\r\n",
        "ratings['movie_index'] = ratings.movie_id.apply(lambda x: movieid_to_movieindex.get(x))\r\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique users: [   1    2    3 ... 6038 6039 6040] 6040\n",
            "Unique items: [   1    2    3 ... 3950 3951 3952] 3706\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>user_index</th>\n",
              "      <th>movie_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  movie_id  rating  user_index  movie_index\n",
              "0        1      1193       5           0         1104\n",
              "1        1       661       3           0          639\n",
              "2        1       914       3           0          853\n",
              "3        1      3408       4           0         3177\n",
              "4        1      2355       5           0         2162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bstO86o4Fq7"
      },
      "source": [
        "посчитаем bias для юзеров и айтемов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVdCWkkM8Hji"
      },
      "source": [
        "bias_user = np.array(ratings.groupby('user_index').rating.mean())\r\n",
        "bias_movie = np.array(ratings.groupby('movie_index').rating.mean())\r\n",
        "assert(len(bias_user) == n_unique_users and len(bias_movie) == n_unique_movies)\r\n",
        "global_bias = ratings.rating.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqUYkAg89zpb"
      },
      "source": [
        "Сделаем explicit данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0qUSgyP9zLx",
        "outputId": "f9f4f4de-ded1-4c3a-e4c8-a33b17ac67d8"
      },
      "source": [
        "data = ratings.to_numpy()\r\n",
        "X_explicit = data[:, 3:5]\r\n",
        "y_explicit = data[:, 2]\r\n",
        "print(X_explicit.shape, y_explicit.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000209, 2) (1000209,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTliPfb6o6Hl"
      },
      "source": [
        "Сделаем implicit данные"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "cbKe9yzxTyd0",
        "outputId": "6febe0fd-c1fb-4838-bf3e-02b6f99df279"
      },
      "source": [
        "ratings[\"implicit\"] = (ratings.rating >= 4).astype('int32')\r\n",
        "ratings.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_id</th>\n",
              "      <th>movie_id</th>\n",
              "      <th>rating</th>\n",
              "      <th>user_index</th>\n",
              "      <th>movie_index</th>\n",
              "      <th>implicit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1193</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1104</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>661</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>639</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>914</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>853</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>3408</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3177</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>2355</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>2162</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user_id  movie_id  rating  user_index  movie_index  implicit\n",
              "0        1      1193       5           0         1104         1\n",
              "1        1       661       3           0          639         0\n",
              "2        1       914       3           0          853         0\n",
              "3        1      3408       4           0         3177         1\n",
              "4        1      2355       5           0         2162         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mm6ZDA-pUdm"
      },
      "source": [
        "X_implicit = np.zeros((len(unique_users), len(unique_movies)))\r\n",
        "X_implicit[ratings.iloc[:, 3], ratings.iloc[:, 4]] = ratings.implicit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnv8wuFf84fz"
      },
      "source": [
        "История просмотров пользователя"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPdUwi2x860V"
      },
      "source": [
        "def get_user_history(user_id):\r\n",
        "  return [movie_info[movie_info.movie_id == x].name.to_string()\r\n",
        "          for x in ratings[(ratings.user_id == user_id) & (ratings.rating >= 4)].movie_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6oXLgvGEaWz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d32e8277-4737-4354-ab7e-f5c9a676515a"
      },
      "source": [
        "get_user_history(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3399    Hustler, The (1961)',\n",
              " '2882    Fistful of Dollars, A (1964)',\n",
              " '1196    Alien (1979)',\n",
              " '1023    Die Hard (1988)',\n",
              " '257    Star Wars: Episode IV - A New Hope (1977)',\n",
              " '1959    Saving Private Ryan (1998)',\n",
              " '476    Jurassic Park (1993)',\n",
              " '1180    Raiders of the Lost Ark (1981)',\n",
              " '1885    Rocky (1976)',\n",
              " '1081    E.T. the Extra-Terrestrial (1982)',\n",
              " '3349    Thelma & Louise (1991)',\n",
              " '3633    Mad Max (1979)',\n",
              " '2297    King Kong (1933)',\n",
              " '1366    Jaws (1975)',\n",
              " '1183    Good, The Bad and The Ugly, The (1966)',\n",
              " '2623    Run Lola Run (Lola rennt) (1998)',\n",
              " '2878    Goldfinger (1964)',\n",
              " '1220    Terminator, The (1984)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn40zPStDidR"
      },
      "source": [
        "Поиск похожих фильмов\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5K7RhaEom0"
      },
      "source": [
        "def get_similars(movie_id, movie_embeddings, k=10, mode=\"dot\"):\r\n",
        "  movie_index = movieid_to_movieindex.get(movie_id)\r\n",
        "  if mode == \"dot\":\r\n",
        "    similar_indexes = np.dot(movie_embeddings, movie_embeddings[movie_index]).argsort()[-k:][::-1]\r\n",
        "  else:\r\n",
        "    similarity = np.dot(movie_embeddings, movie_embeddings[movie_index]) / np.sqrt((movie_embeddings * movie_embeddings).sum(axis=1)) / np.sqrt((movie_embeddings[movie_index] ** 2).sum())\r\n",
        "    similar_indexes = similarity.argsort()[-k:][::-1]\r\n",
        "  similar_names = [movie_info[movie_info[\"movie_id\"] == movieindex_to_movieid.get(index)][\"name\"].to_string() for index in similar_indexes]\r\n",
        "  return similar_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XLl8e8PQLhL"
      },
      "source": [
        "Рекомендации для пользователя"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KofoFBbQ6PX"
      },
      "source": [
        "def make_recommendations(user_id, user_embeddings, movie_embeddings, k=10, mode=\"dot\"):\r\n",
        "  user_index = userid_to_userindex.get(user_id)\r\n",
        "  if mode == \"dot\":\r\n",
        "    recommendation_indexes = np.dot(movie_embeddings, user_embeddings[user_index]).argsort()[-k:][::-1]\r\n",
        "  else:\r\n",
        "    similarity = np.dot(movie_embeddings, user_embeddings[user_index]) / np.sqrt((movie_embeddings * movie_embeddings).sum(axis=1)) / np.sqrt((user_embeddings[user_index] ** 2).sum())\r\n",
        "    recommendation_indexes = similarity.argsort()[-k:][::-1]\r\n",
        "  similar_names = [movie_info[movie_info[\"movie_id\"] == movieindex_to_movieid.get(index)][\"name\"].to_string() for index in recommendation_indexes]\r\n",
        "  return similar_names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpaY2rUGBzXs"
      },
      "source": [
        "##Алгоритмы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzR97GR2dHv"
      },
      "source": [
        "###SVD разложение на explicit данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPj6Hx5gleSn"
      },
      "source": [
        "Напишем loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE-VLw2siOHE"
      },
      "source": [
        "def rmse_loss(predictions, ground_truth):\r\n",
        "  return np.sqrt(((predictions - ground_truth) ** 2).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqE-WLcj_42P"
      },
      "source": [
        "Напишем модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXn9z8QY2b0B"
      },
      "source": [
        "class SVD():\r\n",
        "  def xavier_initialization(self, shape):\r\n",
        "    _, d = shape\r\n",
        "    bounde = 1 / d\r\n",
        "    return np.random.uniform(-bounde, bounde, shape)\r\n",
        "\r\n",
        "  def __init__(self, user_bias, item_bias, global_bias, n_users, n_items, embed_size=128, learning_rate=0.001, lr_decay=1, gamma=0):\r\n",
        "    \"\"\"\r\n",
        "    Инициализация модели: эмбединги инициализируются с помощью xavier инициализации, bias'ы средними значениями.\r\n",
        "    \"\"\"\r\n",
        "    self.user_embed = self.xavier_initialization((n_users, embed_size))\r\n",
        "    self.item_embed = self.xavier_initialization((n_items, embed_size))\r\n",
        "    self.user_bias = user_bias.copy()\r\n",
        "    self.item_bias = item_bias.copy()\r\n",
        "    self.global_bias = global_bias\r\n",
        "    self.learning_rate = learning_rate\r\n",
        "    self.lr_decay = lr_decay\r\n",
        "    self.gamma = gamma\r\n",
        "    self.cnt_users = np.zeros(n_users)\r\n",
        "    self.cnt_items = np.zeros(n_items)\r\n",
        "\r\n",
        "  def embeding_lookup(self, X):\r\n",
        "    return self.user_embed[X[:, 0]], self.user_bias[X[:, 0]], \\\r\n",
        "           self.item_embed[X[:, 1]], self.item_bias[X[:, 1]]\r\n",
        "\r\n",
        "  def learning_step(self, X_batch, y_batch):\r\n",
        "    \"\"\"\r\n",
        "    Происходит 1 шаг градиентного спуска: считаются предсказания по примерам из батча, согласно им обновляются параметры и\r\n",
        "    считается текущий loss.\r\n",
        "    \"\"\"\r\n",
        "    predictions = self.predict(X_batch)\r\n",
        "    self.update_params(X_batch, predictions, y_batch)\r\n",
        "    return rmse_loss(predictions, y_batch)\r\n",
        "\r\n",
        "  def predict(self, X_batch):\r\n",
        "    \"\"\"\r\n",
        "    Берутся эмбединги для пользователей и item'ов из текущего батча и делаются предсказания для них.\r\n",
        "    \"\"\"\r\n",
        "    user_embeddings, user_bias, item_embeddings, item_bias = self.embeding_lookup(X_batch)\r\n",
        "    return np.sum(user_embeddings * item_embeddings, axis=1) + user_bias + item_bias + self.global_bias\r\n",
        "\r\n",
        "  def update_params(self, X_batch, predictions, ground_truth):\r\n",
        "    self.cnt_users = np.zeros_like(self.cnt_users)\r\n",
        "    self.cnt_items = np.zeros_like(self.cnt_items)\r\n",
        "\r\n",
        "    self.update_user(X_batch[:, 0], X_batch[:, 1], predictions, ground_truth)\r\n",
        "    self.update_item(X_batch[:, 0], X_batch[:, 1], predictions, ground_truth)\r\n",
        "    \r\n",
        "  def update_user(self, user_indexes, item_indexes, predictions, ground_truth):\r\n",
        "    \"\"\"\r\n",
        "    Считается производная для юзеров по эмбедингам и bias'у и обновляются параметры модели.\r\n",
        "    \"\"\"\r\n",
        "    np.add.at(self.cnt_users, user_indexes, 1)\r\n",
        "    \r\n",
        "    gradient_embed = self.learning_rate * ((predictions - ground_truth)[:, None] * self.item_embed[item_indexes] + self.gamma * self.user_embed[user_indexes])\r\n",
        "    gradient_embed /= self.cnt_users[user_indexes][:, None]\r\n",
        "    np.subtract.at(self.user_embed, user_indexes, gradient_embed)\r\n",
        "\r\n",
        "    gradient_bias = self.learning_rate * (predictions - ground_truth + self.gamma * self.user_bias[user_indexes])\r\n",
        "    gradient_bias /= self.cnt_users[user_indexes]\r\n",
        "    np.subtract.at(self.user_bias, user_indexes, gradient_bias)\r\n",
        "\r\n",
        "  def update_item(self, user_indexes, item_indexes, predictions, ground_truth):\r\n",
        "    \"\"\"\r\n",
        "    Считается производная для item'ов по эмбедингам и bias'у и обновляются параметры модели.\r\n",
        "    \"\"\"\r\n",
        "    np.add.at(self.cnt_items, item_indexes, 1)\r\n",
        "    \r\n",
        "    gradient_embed = self.learning_rate * ((predictions - ground_truth)[:, None] * self.user_embed[user_indexes] + self.gamma * self.item_embed[item_indexes])\r\n",
        "    gradient_embed /= self.cnt_items[item_indexes][:, None]\r\n",
        "    np.subtract.at(self.item_embed, item_indexes, gradient_embed)\r\n",
        "\r\n",
        "    gradient_bias = self.learning_rate * (predictions - ground_truth + self.gamma * self.item_bias[item_indexes])\r\n",
        "    gradient_bias /= self.cnt_items[item_indexes]\r\n",
        "    np.subtract.at(self.item_bias, item_indexes, gradient_bias)\r\n",
        "  \r\n",
        "  def update_lr(self):\r\n",
        "    self.learning_rate *= self.lr_decay"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kKhOyRckeI9"
      },
      "source": [
        "Обучим модель, разобьём данные на тренировочные и тестовые как 0.8 и 0.2, чтобы знать что мы не переобучаемся"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0OQqKtIbynk",
        "outputId": "ced7fbf6-0a25-4427-f4f4-90e6bf40752a"
      },
      "source": [
        "model = SVD(bias_user, bias_movie, global_bias, n_unique_users, n_unique_movies, learning_rate=0.005, lr_decay=0.99, embed_size=64, gamma=0.1)\r\n",
        "batch_size = 16\r\n",
        "n = X_explicit.shape[0]\r\n",
        "p = np.random.permutation(n)\r\n",
        "X_explicit, y_explicit = X_explicit[p], y_explicit[p]\r\n",
        "X_train, X_test = X_explicit[:math.ceil(n * 0.8)], X_explicit[math.ceil(n * 0.8):] # разбивает X_explicit на train и test\r\n",
        "y_train, y_test = y_explicit[:math.ceil(n * 0.8)], y_explicit[math.ceil(n * 0.8):] # разбивает y_explicit на train и test\r\n",
        "n_batches_train = (X_train.shape[0] + batch_size - 1) // batch_size\r\n",
        "  \r\n",
        "epoches = 100 # кол-во эпох\r\n",
        "for epoch in range(epoches):\r\n",
        "  batches_loss = []\r\n",
        "  for i in range(n_batches_train):\r\n",
        "    X_batch, y_batch = X_train[i * batch_size : (i + 1) * batch_size], y_train[i * batch_size : (i + 1) * batch_size] # текущий батч\r\n",
        "    loss = model.learning_step(X_batch, y_batch) # делаем 1 шаг градиентного спуска\r\n",
        "    batches_loss.append(loss)\r\n",
        "  \r\n",
        "  model.update_lr() # после каждой эпохи уменьшаем lr\r\n",
        "  if epoch % 10 == 0 or epoch == epoches - 1:\r\n",
        "    predictions = model.predict(X_test) \r\n",
        "    rmse = rmse_loss(predictions, y_test) # считаем loss на тесте  \r\n",
        "    print(\"Epoch number =\", epoch)\r\n",
        "    print(\"Rmse on train:\", np.mean(batches_loss))\r\n",
        "    print(\"Rmse on test:\", rmse)\r\n",
        "    print(\"=============\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch number = 0\n",
            "Rmse on train: 2.8066177920654085\n",
            "Rmse on test: 1.7603730076505744\n",
            "=============\n",
            "Epoch number = 10\n",
            "Rmse on train: 0.926194099335473\n",
            "Rmse on test: 0.9507247294418009\n",
            "=============\n",
            "Epoch number = 20\n",
            "Rmse on train: 0.8836704838611382\n",
            "Rmse on test: 0.9108449168358468\n",
            "=============\n",
            "Epoch number = 30\n",
            "Rmse on train: 0.8707509823739459\n",
            "Rmse on test: 0.8990872709805094\n",
            "=============\n",
            "Epoch number = 40\n",
            "Rmse on train: 0.86362331879451\n",
            "Rmse on test: 0.8932291632662728\n",
            "=============\n",
            "Epoch number = 50\n",
            "Rmse on train: 0.8576413547245135\n",
            "Rmse on test: 0.8888294546761891\n",
            "=============\n",
            "Epoch number = 60\n",
            "Rmse on train: 0.851935998157206\n",
            "Rmse on test: 0.8849146216922728\n",
            "=============\n",
            "Epoch number = 70\n",
            "Rmse on train: 0.8468138133826649\n",
            "Rmse on test: 0.8815877024701477\n",
            "=============\n",
            "Epoch number = 80\n",
            "Rmse on train: 0.8426042724993952\n",
            "Rmse on test: 0.879005876834848\n",
            "=============\n",
            "Epoch number = 90\n",
            "Rmse on train: 0.8392260386326517\n",
            "Rmse on test: 0.8770462515996386\n",
            "=============\n",
            "Epoch number = 99\n",
            "Rmse on train: 0.8367297026325137\n",
            "Rmse on test: 0.8756688824927151\n",
            "=============\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq99l7zSkcMI"
      },
      "source": [
        "Переобучения особо нет, обучим модель на всех данных используя те же гиперпараметры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VtdBfKp4ZFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbd427c-7d98-411a-c5d4-46223a5b7263"
      },
      "source": [
        "model = SVD(bias_user, bias_movie, global_bias, n_unique_users, n_unique_movies, learning_rate=0.005, lr_decay=0.99, embed_size=64, gamma=0.1)\r\n",
        "n_batches = (X_explicit.shape[0] + batch_size - 1) // batch_size\r\n",
        "\r\n",
        "for epoch in range(epoches):\r\n",
        "  batches_loss = []\r\n",
        "  for i in range(n_batches):\r\n",
        "    X_batch, y_batch = X_explicit[i * batch_size : (i + 1) * batch_size], y_explicit[i * batch_size : (i + 1) * batch_size]\r\n",
        "    loss = model.learning_step(X_batch, y_batch)\r\n",
        "    batches_loss.append(loss)\r\n",
        "  if epoch % 10 == 0 or epoch == epoches - 1:\r\n",
        "    print(epoch, np.mean(batches_loss))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 2.5758649291871785\n",
            "10 0.9040907943093638\n",
            "20 0.8742413395848392\n",
            "30 0.8638414612024338\n",
            "40 0.8538798364089341\n",
            "50 0.845276702693424\n",
            "60 0.8394722497171836\n",
            "70 0.835383214030834\n",
            "80 0.8320209481965833\n",
            "90 0.8290379220293516\n",
            "99 0.8267012456840654\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oT_jfW3GlXKk"
      },
      "source": [
        "Найдём похожие фильмы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rd7Zb7HWa5mp",
        "outputId": "a5643eea-6f68-4278-885a-af73c6effd3f"
      },
      "source": [
        "get_similars(1, model.item_embed, mode=\"cos\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0    Toy Story (1995)',\n",
              " '584    Aladdin (1992)',\n",
              " '3045    Toy Story 2 (1999)',\n",
              " \"2286    Bug's Life, A (1998)\",\n",
              " '591    Beauty and the Beast (1991)',\n",
              " '2728    Big (1988)',\n",
              " '2012    Little Mermaid, The (1989)',\n",
              " '1081    E.T. the Extra-Terrestrial (1982)',\n",
              " '33    Babe (1995)',\n",
              " '2618    Tarzan (1999)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxB7-XJrqev2"
      },
      "source": [
        "Сделаем рекомендации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvLgxDtzbPLF",
        "outputId": "37dcaeca-f257-4bfa-9c2d-d1acc19e58ae"
      },
      "source": [
        "make_recommendations(4, model.user_embed, model.item_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3346    Mirror, The (Zerkalo) (1975)',\n",
              " '1461    Love and Other Catastrophes (1996)',\n",
              " '1147    Mina Tannenbaum (1994)',\n",
              " '3747    Other Side of Sunday, The (S�ndagsengler) (1996)',\n",
              " '2563    Saragossa Manuscript, The (Rekopis znaleziony ...',\n",
              " '3746    Official Story, The (La Historia Oficial) (1985)',\n",
              " '104    Nobody Loves Me (Keiner liebt mich) (1994)',\n",
              " '2761    Cabaret Balkan (Bure Baruta) (1998)',\n",
              " '3576    Cleo From 5 to 7 (Cl�o de 5 � 7) (1962)',\n",
              " '2169    Seven Beauties (Pasqualino Settebellezze) (1976)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vt4gpaFGsz5Q"
      },
      "source": [
        "###ALS разложение на implicit данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxode0pCF0lh"
      },
      "source": [
        "Напишем взвешенных loss, который штрафует сильнее за предсказания 0 на месте 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00DCAM-V3Co5"
      },
      "source": [
        "def weighted_rmse(predictions, ground_truth, alpha):\r\n",
        "  weights = 1 + alpha * ground_truth\r\n",
        "  return np.sqrt((weights * (predictions - ground_truth) ** 2).mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VdQOf9EAtyK"
      },
      "source": [
        "class ALS():\r\n",
        "  def xavier_initialization(self, shape):\r\n",
        "    _, d = shape\r\n",
        "    bounde = 1 / d\r\n",
        "    return np.random.uniform(-bounde, bounde, shape)\r\n",
        "  \r\n",
        "  def __init__(self, data, embed_size, n_steps=10, alpha=100, gamma=0.25):\r\n",
        "    self.n_users, self.n_items = data.shape\r\n",
        "    self.user_embed = self.xavier_initialization((self.n_users, embed_size))\r\n",
        "    self.item_embed = self.xavier_initialization((self.n_items, embed_size))\r\n",
        "    self.data = data\r\n",
        "    self.c = 1 + alpha * data\r\n",
        "    self.n_steps = n_steps\r\n",
        "    self.alpha = alpha\r\n",
        "    self.gamma = gamma\r\n",
        "    self.eye = np.eye(embed_size)\r\n",
        "    self.eye_items = np.eye(self.n_items)\r\n",
        "    self.eye_users = np.eye(self.n_users)\r\n",
        "  \r\n",
        "  def fit(self):\r\n",
        "    \"\"\"\r\n",
        "    Обучение модели: @self.n_steps раз делается обновление параметров согласно @update_params.\r\n",
        "    \"\"\"\r\n",
        "    for i in range(self.n_steps):\r\n",
        "      predictions = self.predict()\r\n",
        "      start = time.time()\r\n",
        "      self.update_params()\r\n",
        "      print(\"Step\", i, \"loss =\", weighted_rmse(predictions, self.data, self.alpha), \"time took\", time.time() - start)\r\n",
        "\r\n",
        "  def predict(self):\r\n",
        "    \"\"\"\r\n",
        "    Предсказания делаются на основе скалярного произведения.\r\n",
        "    \"\"\"\r\n",
        "    return np.dot(self.user_embed, self.item_embed.T)\r\n",
        "  \r\n",
        "  def update_params(self):\r\n",
        "    \"\"\"\r\n",
        "    Обновления весов описанное в статье http://yifanhu.net/PUB/cf.pdf.\r\n",
        "    \"\"\"\r\n",
        "    tmp_item = np.dot(self.item_embed.T, self.item_embed)\r\n",
        "    for user in range(self.n_users):\r\n",
        "      nonzero = (self.c[user] - 1).astype(bool) # ускорение матричного произведения, благодаря которому оно работает за O(f^2 * n_u), а не O(f^2 * n), где n_u - кол-во не нулевых элементов.\r\n",
        "      iTi = self.alpha * self.item_embed.T[:, nonzero] @ self.item_embed[nonzero]\r\n",
        "      self.user_embed[user] = np.linalg.inv(tmp_item + iTi + self.gamma * self.eye) @ self.item_embed.T @ (self.c[user] * self.data[user])\r\n",
        "    \r\n",
        "    tmp_user = np.dot(self.user_embed.T, self.user_embed)\r\n",
        "    for item in range(self.n_items):\r\n",
        "      nonzero = (self.c[:, item] - 1).astype(bool)\r\n",
        "      uTu = self.alpha * self.user_embed.T[:, nonzero] @ self.user_embed[nonzero]\r\n",
        "      self.item_embed[item] = np.linalg.inv(tmp_user + uTu + self.gamma * self.eye) @ self.user_embed.T @ (self.c[:, item] * self.data[:, item])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtDUtx0bIKqX",
        "outputId": "f5914fbe-118c-4d15-eedf-e3ec7c7a618d"
      },
      "source": [
        "model = ALS(X_implicit, n_steps=10, alpha=10, embed_size=64, gamma=0.25)\r\n",
        "model.fit()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step 0 loss = 0.5316992137675064 time took 13.28004789352417\n",
            "Step 1 loss = 0.30439773104354234 time took 13.593106746673584\n",
            "Step 2 loss = 0.2541990150861876 time took 13.603131771087646\n",
            "Step 3 loss = 0.2473237645661728 time took 13.743133306503296\n",
            "Step 4 loss = 0.24468871035175582 time took 14.520085334777832\n",
            "Step 5 loss = 0.24337373539891827 time took 13.922136068344116\n",
            "Step 6 loss = 0.2426129692543914 time took 13.81650447845459\n",
            "Step 7 loss = 0.24212919544706354 time took 14.088450908660889\n",
            "Step 8 loss = 0.24180123436320294 time took 13.97995924949646\n",
            "Step 9 loss = 0.2415687019347711 time took 13.868547201156616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttwpwO3ybVvW"
      },
      "source": [
        "Найдём похожие фильмы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IW7ZJFXjbVMl",
        "outputId": "7dbc24cd-3f16-4673-c714-e5e4df30b08e"
      },
      "source": [
        "get_similars(1, model.item_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3045    Toy Story 2 (1999)',\n",
              " '0    Toy Story (1995)',\n",
              " \"2286    Bug's Life, A (1998)\",\n",
              " '33    Babe (1995)',\n",
              " '584    Aladdin (1992)',\n",
              " '1245    Groundhog Day (1993)',\n",
              " '591    Beauty and the Beast (1991)',\n",
              " '360    Lion King, The (1994)',\n",
              " '2252    Pleasantville (1998)',\n",
              " \"1854    There's Something About Mary (1998)\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb2161nmbaVR"
      },
      "source": [
        "Сделаем рекомендации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTDpyoclbdMD",
        "outputId": "796e4753-7630-4484-a39a-a5a5b1767bb6"
      },
      "source": [
        "make_recommendations(4, model.user_embed, model.item_embed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['257    Star Wars: Episode IV - A New Hope (1977)',\n",
              " '1220    Terminator, The (1984)',\n",
              " '1180    Raiders of the Lost Ark (1981)',\n",
              " '1196    Alien (1979)',\n",
              " '1366    Jaws (1975)',\n",
              " '585    Terminator 2: Judgment Day (1991)',\n",
              " '1178    Star Wars: Episode V - The Empire Strikes Back...',\n",
              " '2502    Matrix, The (1999)',\n",
              " '1183    Good, The Bad and The Ugly, The (1966)',\n",
              " '1023    Die Hard (1988)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S88djHbkBQXI"
      },
      "source": [
        "###BPR на implicit данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FkzHlm3GGUo"
      },
      "source": [
        "Сделаем массив set'ов id фильмов, которые понравились user'ам, чтобы при negative sampling'е быстро понимать отрицательный ли данных пример."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVkK-BFIBjPf"
      },
      "source": [
        "users_pos_samples = []\r\n",
        "for i, user in enumerate(X_implicit):\r\n",
        "  indexes_pos = set(np.where(user == 1)[0])\r\n",
        "  users_pos_samples.append(indexes_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fOB4y6Gibh"
      },
      "source": [
        "Сделаем dataset, размер которого равен кол-во отрицательных примеров, умноженное на суммарное кол-во фильмов которые понравились всем пользователям."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrTEV53xDYvs"
      },
      "source": [
        "class BPRDataset(Dataset):\r\n",
        "  def __init__(self, users_pos_samples, n_items, n_negative_samples=3):\r\n",
        "    self.users_pos_samples = users_pos_samples\r\n",
        "    self.n_items = n_items\r\n",
        "    self.n_users = len(users_pos_samples)\r\n",
        "    self.data = [] # массив пар (user_index, movie_index), где movie_index - положительный пример для user_index\r\n",
        "    for user_index, pos_samples in enumerate(users_pos_samples):\r\n",
        "      for pos_sample in pos_samples:\r\n",
        "        self.data.append([user_index, pos_sample])\r\n",
        "    self.n_negative_samples = n_negative_samples\r\n",
        "    \r\n",
        "  def __len__(self):\r\n",
        "    return self.n_negative_samples * len(self.data)\r\n",
        "\r\n",
        "  def __getitem__(self, id):\r\n",
        "    user, pos_sample = self.data[id // self.n_negative_samples]\r\n",
        "    while True:\r\n",
        "      neg_sample = np.random.randint(0, self.n_items) # random negative sampling\r\n",
        "      if neg_sample not in self.users_pos_samples[user]: # если пример действительно негативный, то тройка найдена\r\n",
        "        break\r\n",
        "    return (user, pos_sample, neg_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrlS0yWlp93J"
      },
      "source": [
        "batch_size = 64\r\n",
        "bpr_dataset = BPRDataset(users_pos_samples, n_unique_movies, n_negative_samples=10)\r\n",
        "bpr_dataloader = DataLoader(bpr_dataset, batch_size=batch_size, shuffle=True, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox0IVY4mG3Sm"
      },
      "source": [
        "Сделаем модель, которая возвращает пару - score эмбединга пользователя с эмбидингом положительным и отрицательным примерами."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVY3-2npLoJ9"
      },
      "source": [
        "class DotModel(nn.Module):\r\n",
        "  def __init__(self, n_users, n_items, embed_size):\r\n",
        "    super(DotModel, self).__init__()\r\n",
        "    self.user_embed = nn.Embedding(n_users, embed_size)\r\n",
        "    self.item_embed = nn.Embedding(n_items, embed_size)\r\n",
        "    \r\n",
        "    sqrt_embed_size = np.sqrt(embed_size)\r\n",
        "    self.user_embed.weight.data.uniform_(-1 / sqrt_embed_size, 1 / sqrt_embed_size)\r\n",
        "    self.item_embed.weight.data.uniform_(-1 / sqrt_embed_size, 1 / sqrt_embed_size)\r\n",
        "    \r\n",
        "  \r\n",
        "  def forward(self, users, pos_samples, neg_samples):\r\n",
        "    user_embed = self.user_embed(users)\r\n",
        "    pos_items = self.item_embed(pos_samples)\r\n",
        "    neg_items = self.item_embed(neg_samples)\r\n",
        "    return (torch.sum(user_embed * pos_items, axis=1), torch.sum(user_embed * neg_items, axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "top5B-mUfbVa"
      },
      "source": [
        "Loss из оригинальной статьи, где в качестве \\hat{x}_{uij} используется разность похожести эмбединга пользователя с эмбедингом положительного примера и с эмбедингом отрицательного примера.  \r\n",
        "Вместо максимизации правдоподобия делается минимизация минус правдоподобия."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfeJxYcx8VDM"
      },
      "source": [
        "def bpr_loss(pos_dots, neg_dots):\r\n",
        "  return -torch.log(torch.sigmoid(pos_dots - neg_dots)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHWhnMQ_gGJ8"
      },
      "source": [
        "Функция обучения заданной модели на заданном dataloader'е."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkG9lS2Tqpcd"
      },
      "source": [
        "def train_model(model, dataloader, n_epoches=3, lr=0.001, weight_decay=0, verbose_every=10000):\r\n",
        "  opt = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\r\n",
        "  \r\n",
        "  for epoch in range(n_epoches):\r\n",
        "    batch_loss = []\r\n",
        "    start = time.time()\r\n",
        "    for i, batch in enumerate(dataloader):\r\n",
        "      opt.zero_grad()\r\n",
        "      users, pos_samples, neg_samples = batch\r\n",
        "      pos_dots, neg_dots = model(users, pos_samples, neg_samples)\r\n",
        "      loss = bpr_loss(pos_dots, neg_dots)\r\n",
        "      loss.backward()\r\n",
        "      opt.step()\r\n",
        "      batch_loss.append(loss.item())\r\n",
        "      if i % verbose_every == 0:\r\n",
        "        print(\"epoch =\", epoch, \"batch =\", i, \"loss =\", np.mean(batch_loss), \"time took =\", time.time() - start)\r\n",
        "        start = time.time()\r\n",
        "        batch_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxMAX9UlHMB9"
      },
      "source": [
        "Обучим модель на BPR данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6-1VbpIq38U",
        "outputId": "97fc3650-f6ce-4668-8676-9aa21e46491c"
      },
      "source": [
        "model = DotModel(n_unique_users, n_unique_movies, 64)\r\n",
        "train_model(model, bpr_dataloader, verbose_every=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 batch = 0 loss = 0.6939931511878967 time took = 0.631885290145874\n",
            "epoch = 0 batch = 10000 loss = 0.40493274340629576 time took = 70.85172295570374\n",
            "epoch = 0 batch = 20000 loss = 0.21643283905759453 time took = 72.04491329193115\n",
            "epoch = 0 batch = 30000 loss = 0.17922773022688926 time took = 71.4583694934845\n",
            "epoch = 0 batch = 40000 loss = 0.15973469309434293 time took = 69.06842970848083\n",
            "epoch = 0 batch = 50000 loss = 0.14473877490162848 time took = 70.42462277412415\n",
            "epoch = 0 batch = 60000 loss = 0.13368620417378843 time took = 69.87066149711609\n",
            "epoch = 0 batch = 70000 loss = 0.12680612712372094 time took = 70.45058274269104\n",
            "epoch = 0 batch = 80000 loss = 0.1185579035161063 time took = 69.58873796463013\n",
            "epoch = 1 batch = 0 loss = 0.12277768552303314 time took = 0.6208419799804688\n",
            "epoch = 1 batch = 10000 loss = 0.10490929673984647 time took = 74.9869704246521\n",
            "epoch = 1 batch = 20000 loss = 0.10007667351877317 time took = 76.2795901298523\n",
            "epoch = 1 batch = 30000 loss = 0.0980128707469441 time took = 77.65115666389465\n",
            "epoch = 1 batch = 40000 loss = 0.09556288096997886 time took = 73.040762424469\n",
            "epoch = 1 batch = 50000 loss = 0.09327964609134942 time took = 72.70725011825562\n",
            "epoch = 1 batch = 60000 loss = 0.09138583204532043 time took = 75.09038639068604\n",
            "epoch = 1 batch = 70000 loss = 0.0880641631364124 time took = 75.90997815132141\n",
            "epoch = 1 batch = 80000 loss = 0.08750922801322304 time took = 74.65443348884583\n",
            "epoch = 2 batch = 0 loss = 0.0707939937710762 time took = 0.6281847953796387\n",
            "epoch = 2 batch = 10000 loss = 0.08229786840742453 time took = 76.3591239452362\n",
            "epoch = 2 batch = 20000 loss = 0.08097840596851892 time took = 76.0870521068573\n",
            "epoch = 2 batch = 30000 loss = 0.08077433526169044 time took = 76.30602431297302\n",
            "epoch = 2 batch = 40000 loss = 0.080854434322624 time took = 74.14517712593079\n",
            "epoch = 2 batch = 50000 loss = 0.07974652135417563 time took = 75.22817325592041\n",
            "epoch = 2 batch = 60000 loss = 0.0796708861699677 time took = 75.11209154129028\n",
            "epoch = 2 batch = 70000 loss = 0.07934131209620973 time took = 73.8843412399292\n",
            "epoch = 2 batch = 80000 loss = 0.07871096030006301 time took = 76.09602665901184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Zwc8msHSGK"
      },
      "source": [
        "Посмотрим на похожие фильмы и рекомендации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndJhQ7U2BY1Y",
        "outputId": "3fdfdf01-d7f4-402b-a9a1-d5e2d1ce667a"
      },
      "source": [
        "get_similars(1, model.item_embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0    Toy Story (1995)',\n",
              " '584    Aladdin (1992)',\n",
              " '360    Lion King, The (1994)',\n",
              " '591    Beauty and the Beast (1991)',\n",
              " '1526    Hercules (1997)',\n",
              " \"2286    Bug's Life, A (1998)\",\n",
              " '2012    Little Mermaid, The (1989)',\n",
              " '773    Hunchback of Notre Dame, The (1996)',\n",
              " '3045    Toy Story 2 (1999)',\n",
              " '590    Snow White and the Seven Dwarfs (1937)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzQDKm6z_KnU",
        "outputId": "8dbf618a-2fb4-4e48-9302-e253f99e1b80"
      },
      "source": [
        "make_recommendations(4, model.user_embed.weight.data, model.item_embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['257    Star Wars: Episode IV - A New Hope (1977)',\n",
              " '847    Godfather, The (1972)',\n",
              " '1180    Raiders of the Lost Ark (1981)',\n",
              " '1196    Alien (1979)',\n",
              " '1192    Star Wars: Episode VI - Return of the Jedi (1983)',\n",
              " '1366    Jaws (1975)',\n",
              " '1271    Indiana Jones and the Last Crusade (1989)',\n",
              " '1183    Good, The Bad and The Ugly, The (1966)',\n",
              " '1182    Aliens (1986)',\n",
              " '2878    Goldfinger (1964)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7Z3O2JqfERS"
      },
      "source": [
        "### WARP на implicit данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oP-gTklINQ"
      },
      "source": [
        "Напишем другой датасет, модель оставим такую же, потому что изменяется только то, на каких примерах мы её учим.  \r\n",
        "В датасете будем делать 1 negative sample, но не рандомный, а тот на котором модель работает плохо - предсказывает негативный пример более похожим чем позитивный, установим ограничение на время поиска - max_samples, чтобы можно было учить больше эпох, это улучшает общее качество модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNqCsxtoVV9v"
      },
      "source": [
        "class WARPDataset(Dataset):\r\n",
        "  def __init__(self, users_pos_samples, n_items, user_embed, item_embed, max_samples=100):\r\n",
        "    self.users_pos_samples = users_pos_samples\r\n",
        "    self.n_items = n_items\r\n",
        "    self.n_users = len(users_pos_samples)\r\n",
        "    self.user_embed = user_embed\r\n",
        "    self.item_embed = item_embed\r\n",
        "    self.max_samples = max_samples\r\n",
        "    self.data = [] # аналогично BPR\r\n",
        "    for user_index, pos_samples in enumerate(users_pos_samples):\r\n",
        "      for pos_sample in pos_samples:\r\n",
        "        self.data.append([user_index, pos_sample])\r\n",
        "    \r\n",
        "  def __len__(self):\r\n",
        "    return len(self.data)\r\n",
        "\r\n",
        "  def __getitem__(self, id):\r\n",
        "    user, pos_sample = self.data[id]\r\n",
        "    cnt = 0 # счётчик кол-ва негативный примеров, которые сравнивались с позитивным примером pos_sample\r\n",
        "    user_embed = self.user_embed(torch.Tensor([user]).long())\r\n",
        "    pos_embed = self.item_embed(torch.Tensor([pos_sample]).long())\r\n",
        "    while True:\r\n",
        "      neg_sample = np.random.randint(0, self.n_items) # random negative sampling \r\n",
        "      if neg_sample not in self.users_pos_samples[user]:\r\n",
        "        cnt += 1\r\n",
        "        neg_score = self.item_embed(torch.Tensor([neg_sample]).long())\r\n",
        "        if torch.sum(user_embed * pos_embed) < torch.sum(user_embed * neg_score) or cnt == self.max_samples: # проверка на то, что модель работает на этой паре плохо\r\n",
        "          break\r\n",
        "    return (user, pos_sample, neg_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "todLK2tkqEkD",
        "outputId": "a47e171d-a213-4e29-b3bf-9c00955efa4e"
      },
      "source": [
        "model = DotModel(n_unique_users, n_unique_movies, 64)\r\n",
        "\r\n",
        "warp_dataset = WARPDataset(users_pos_samples, n_unique_movies, model.user_embed, model.item_embed)\r\n",
        "warp_dataloader = DataLoader(warp_dataset, batch_size=64, shuffle=True, num_workers=4)\r\n",
        "\r\n",
        "train_model(model, warp_dataloader, n_epoches=20, verbose_every=4000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch = 0 batch = 0 loss = 0.707985520362854 time took = 0.2880270481109619\n",
            "epoch = 0 batch = 4000 loss = 0.5802115963697434 time took = 95.40622210502625\n",
            "epoch = 0 batch = 8000 loss = 0.2926865451782942 time took = 94.91562247276306\n",
            "epoch = 1 batch = 0 loss = 1.0930906534194946 time took = 0.6948308944702148\n",
            "epoch = 1 batch = 4000 loss = 0.5926576810181141 time took = 422.8968026638031\n",
            "epoch = 1 batch = 8000 loss = 0.48400610926747323 time took = 423.36377692222595\n",
            "epoch = 2 batch = 0 loss = 0.9128168225288391 time took = 0.5136561393737793\n",
            "epoch = 2 batch = 4000 loss = 0.4557212028503418 time took = 281.3136899471283\n",
            "epoch = 2 batch = 8000 loss = 0.34209805617854 time took = 287.29824328422546\n",
            "epoch = 3 batch = 0 loss = 1.0808762311935425 time took = 0.8863968849182129\n",
            "epoch = 3 batch = 4000 loss = 0.5296302679032088 time took = 464.7663915157318\n",
            "epoch = 3 batch = 8000 loss = 0.39570797718688844 time took = 461.7753691673279\n",
            "epoch = 4 batch = 0 loss = 0.9408712387084961 time took = 0.6284587383270264\n",
            "epoch = 4 batch = 4000 loss = 0.449534393530339 time took = 380.82449984550476\n",
            "epoch = 4 batch = 8000 loss = 0.337818193834275 time took = 384.43702840805054\n",
            "epoch = 5 batch = 0 loss = 0.9661164283752441 time took = 0.7014658451080322\n",
            "epoch = 5 batch = 4000 loss = 0.48428779178112746 time took = 508.4587528705597\n",
            "epoch = 5 batch = 8000 loss = 0.3642832404598594 time took = 502.61161756515503\n",
            "epoch = 6 batch = 0 loss = 0.8813973665237427 time took = 0.8599612712860107\n",
            "epoch = 6 batch = 4000 loss = 0.430933690559119 time took = 440.97042751312256\n",
            "epoch = 6 batch = 8000 loss = 0.3237413830123842 time took = 448.8866913318634\n",
            "epoch = 7 batch = 0 loss = 1.1212430000305176 time took = 0.8312814235687256\n",
            "epoch = 7 batch = 4000 loss = 0.4619756852015853 time took = 554.3198046684265\n",
            "epoch = 7 batch = 8000 loss = 0.34632924252748487 time took = 542.2731838226318\n",
            "epoch = 8 batch = 0 loss = 0.7828723192214966 time took = 0.7079923152923584\n",
            "epoch = 8 batch = 4000 loss = 0.4219350727237761 time took = 485.1914129257202\n",
            "epoch = 8 batch = 8000 loss = 0.31812183013372125 time took = 485.1496515274048\n",
            "epoch = 9 batch = 0 loss = 1.001009464263916 time took = 0.9092240333557129\n",
            "epoch = 9 batch = 4000 loss = 0.4500030696950853 time took = 568.0212731361389\n",
            "epoch = 9 batch = 8000 loss = 0.3380111373104155 time took = 564.2751479148865\n",
            "epoch = 10 batch = 0 loss = 0.8302491903305054 time took = 0.8147423267364502\n",
            "epoch = 10 batch = 4000 loss = 0.41721591256186363 time took = 504.9179961681366\n",
            "epoch = 10 batch = 8000 loss = 0.3099087495058775 time took = 504.1494698524475\n",
            "epoch = 11 batch = 0 loss = 0.9595162868499756 time took = 0.7987747192382812\n",
            "epoch = 11 batch = 4000 loss = 0.4417133042886853 time took = 573.5721154212952\n",
            "epoch = 11 batch = 8000 loss = 0.3317478977665305 time took = 574.1558308601379\n",
            "epoch = 12 batch = 0 loss = 0.7940491437911987 time took = 0.71425461769104\n",
            "epoch = 12 batch = 4000 loss = 0.4139115937128663 time took = 520.4054152965546\n",
            "epoch = 12 batch = 8000 loss = 0.3085342983175069 time took = 530.8058369159698\n",
            "epoch = 13 batch = 0 loss = 0.8661879897117615 time took = 0.8019504547119141\n",
            "epoch = 13 batch = 4000 loss = 0.4352102360650897 time took = 580.3321349620819\n",
            "epoch = 13 batch = 8000 loss = 0.32550762467086314 time took = 586.2232856750488\n",
            "epoch = 14 batch = 0 loss = 0.8568957448005676 time took = 0.8419573307037354\n",
            "epoch = 14 batch = 4000 loss = 0.41070779285952447 time took = 532.7875580787659\n",
            "epoch = 14 batch = 8000 loss = 0.3097578744739294 time took = 532.6896123886108\n",
            "epoch = 15 batch = 0 loss = 1.0739210844039917 time took = 0.7346692085266113\n",
            "epoch = 15 batch = 4000 loss = 0.42904824214056136 time took = 591.4965827465057\n",
            "epoch = 15 batch = 8000 loss = 0.32362745881825683 time took = 595.4401044845581\n",
            "epoch = 16 batch = 0 loss = 0.9498341083526611 time took = 0.8464760780334473\n",
            "epoch = 16 batch = 4000 loss = 0.409604707961902 time took = 540.84037566185\n",
            "epoch = 16 batch = 8000 loss = 0.3046701730564237 time took = 544.9827580451965\n",
            "epoch = 17 batch = 0 loss = 0.7549999356269836 time took = 0.7972548007965088\n",
            "epoch = 17 batch = 4000 loss = 0.4256535071842372 time took = 604.7907371520996\n",
            "epoch = 17 batch = 8000 loss = 0.318122051179409 time took = 600.5148510932922\n",
            "epoch = 18 batch = 0 loss = 1.0517009496688843 time took = 0.7640335559844971\n",
            "epoch = 18 batch = 4000 loss = 0.4077820124998689 time took = 549.0435893535614\n",
            "epoch = 18 batch = 8000 loss = 0.30268147405050694 time took = 547.1030924320221\n",
            "epoch = 19 batch = 0 loss = 0.7996559739112854 time took = 0.9054081439971924\n",
            "epoch = 19 batch = 4000 loss = 0.42335225484520195 time took = 591.7355408668518\n",
            "epoch = 19 batch = 8000 loss = 0.31582649611867963 time took = 594.9820532798767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7StdEse0U3rf",
        "outputId": "795d9430-b8c3-4ff1-8504-e05c1e6ba687"
      },
      "source": [
        "get_similars(1, model.item_embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['0    Toy Story (1995)',\n",
              " '591    Beauty and the Beast (1991)',\n",
              " '2692    Iron Giant, The (1999)',\n",
              " '2502    Matrix, The (1999)',\n",
              " '1526    Hercules (1997)',\n",
              " '2225    Antz (1998)',\n",
              " '3045    Toy Story 2 (1999)',\n",
              " \"523    Schindler's List (1993)\",\n",
              " '584    Aladdin (1992)',\n",
              " '1262    Fantasia (1940)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLPyCFSaU8qO",
        "outputId": "190bc700-c038-46a8-b961-967d2af7f9e7"
      },
      "source": [
        "make_recommendations(4, model.user_embed.weight.data, model.item_embed.weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2878    Goldfinger (1964)',\n",
              " '1183    Good, The Bad and The Ugly, The (1966)',\n",
              " '3633    Mad Max (1979)',\n",
              " '1950    Seven Samurai (The Magnificent Seven) (Shichin...',\n",
              " '1196    Alien (1979)',\n",
              " '2882    Fistful of Dollars, A (1964)',\n",
              " '2502    Matrix, The (1999)',\n",
              " '2916    Robocop (1987)',\n",
              " '2875    Dirty Dozen, The (1967)',\n",
              " '257    Star Wars: Episode IV - A New Hope (1977)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    }
  ]
}